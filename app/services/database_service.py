"""Helper functions for direct database interactions - Refactored Version."""

import logging
from datetime import datetime
from typing import Any, Dict, List, Optional, Callable
from dataclasses import dataclass
from contextlib import contextmanager
import time
import os

import pandas as pd
from sqlmodel import select, func, Session, delete
from app.etl import convert_numpy_types
from app.models import Experiment, Measurement, ProcessedFile, Step
from app.utils.data_helpers import convert_datetime_to_python
from app.utils.database import get_session as get_db_session
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from sqlalchemy.pool import StaticPool

# é…ç½®
@dataclass
class ProcessingConfig:
    """è™•ç†é…ç½®é¡"""
    default_batch_size: int = 1000
    measurement_internal_batch_size: int = 500  # æ–°å¢ï¼šç”¨æ–¼æ¸¬é‡æ•¸æ“šå…§éƒ¨å°æ‰¹æ¬¡çš„å¤§å°
    max_retry_attempts: int = 3
    voltage_precision: int = 3
    current_precision: int = 3
    temperature_precision: int = 1
    capacity_precision: int = 3
    energy_precision: int = 3
    soc_precision: int = 1
    default_temperature: float = 25.0

# å…¨åŸŸé…ç½®å¯¦ä¾‹
config = ProcessingConfig()

# è¨­ç½®æ—¥èªŒ
logger = logging.getLogger(__name__)

# å®šç¾©æ–‡ä»¶é¡å‹å¸¸é‡
FILE_TYPE_STEP = "step"
FILE_TYPE_DETAIL = "detail"

# è‡ªå®šç¾©ç•°å¸¸é¡åˆ¥
class DatabaseError(Exception):
    """æ•¸æ“šåº«æ“ä½œç•°å¸¸"""
    pass

class ValidationError(Exception):
    """æ•¸æ“šé©—è­‰ç•°å¸¸"""
    pass

class ProcessingError(Exception):
    """æ•¸æ“šè™•ç†ç•°å¸¸"""
    pass

# ä¿®æ”¹å¼•æ“é…ç½®ï¼Œæ·»åŠ é€£ç·šæ± è¨­å®š  
from app.utils.config import DATABASE_URL

engine = create_engine(
    DATABASE_URL,
    poolclass=StaticPool,
    pool_pre_ping=True,
    pool_recycle=300,
    connect_args={
        "check_same_thread": False,
        "timeout": 30,  # å¢åŠ è¶…æ™‚æ™‚é–“
        "isolation_level": None  # è‡ªå‹•æäº¤æ¨¡å¼
    },
    echo=False
)

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# è¼”åŠ©å‡½æ•¸
def retry_on_failure(max_attempts: int = 3, delay: float = 1.0):
    """é‡è©¦è£é£¾å™¨"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            for attempt in range(max_attempts):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_attempts - 1:
                        raise DatabaseError(f"æ“ä½œå¤±æ•—ï¼Œå·²é‡è©¦ {max_attempts} æ¬¡: {str(e)}")
                    logger.warning(f"å˜—è©¦ {attempt + 1} å¤±æ•—ï¼Œæ­£åœ¨é‡è©¦: {str(e)}")
                    if delay > 0:
                        time.sleep(delay)
            return None
        return wrapper
    return decorator

@contextmanager
def safe_session():
    """å®‰å…¨çš„è³‡æ–™åº«æœƒè©±ç®¡ç†"""
    session = SessionLocal()
    try:
        yield session
        session.commit()
    except Exception as e:
        session.rollback()
        raise
    finally:
        session.close()

def retry_database_operation(operation_func, max_retries=3, retry_delay=0.5):
    """é‡è©¦è³‡æ–™åº«æ“ä½œçš„è£é£¾å™¨å‡½æ•¸"""
    for attempt in range(max_retries):
        try:
            return operation_func()
        except Exception as e:
            if "database is locked" in str(e) and attempt < max_retries - 1:
                logger.warning(f"è³‡æ–™åº«è¢«é–å®šï¼Œç­‰å¾…é‡è©¦ ({attempt + 1}/{max_retries})ï¼ŒéŒ¯èª¤: {str(e)}")
                time.sleep(retry_delay * (attempt + 1))  # éå¢ç­‰å¾…æ™‚é–“
                continue
            else:
                raise e
    
    raise DatabaseError("è³‡æ–™åº«æ“ä½œè¶…éæœ€å¤§é‡è©¦æ¬¡æ•¸")

def validate_required_columns(df: pd.DataFrame, required_columns: List[str]) -> None:
    """é©—è­‰å¿…è¦æ¬„ä½"""
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        raise ValidationError(f"ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_columns}")

def safe_get_float_from_dict(d: Dict[str, Any], key: str, default: float = 0.0) -> float:
    """å®‰å…¨åœ°å¾å­—å…¸ä¸­ç²å–æµ®é»æ•¸å€¼"""
    try:
        v = d.get(key, default)
        return float(v) if v is not None else default
    except (ValueError, TypeError):
        return default

def safe_get_str_from_dict(d: Dict[str, Any], key: str, default: str = "") -> str:
    """å®‰å…¨åœ°å¾å­—å…¸ä¸­ç²å–å­—ä¸²"""
    try:
        v = d.get(key, default)
        return str(v) if v is not None else default
    except (ValueError, TypeError):
        return default

def safe_get_optional_float_from_dict(d: Dict[str, Any], key: str) -> Optional[float]:
    """å®‰å…¨åœ°å¾å­—å…¸ä¸­ç²å–å¯é¸çš„æµ®é»æ•¸å€¼"""
    try:
        v = d.get(key)
        return float(v) if v is not None else None
    except (ValueError, TypeError):
        return None

def round_numeric_value(value: Any, precision: int, default: float = 0.0) -> float:
    """å®‰å…¨åœ°å››æ¨äº”å…¥æ•¸å€¼"""
    try:
        if pd.isna(value):
            return default
        return round(float(value), precision)
    except (ValueError, TypeError):
        return default

def calculate_c_rate(current: float, nominal_capacity: float) -> float:
    """è¨ˆç®— C-rate"""
    try:
        return abs(current) / nominal_capacity if nominal_capacity else 0.0
    except (ValueError, TypeError, ZeroDivisionError):
        return 0.0

# ä¸»è¦åŠŸèƒ½å‡½æ•¸
@retry_on_failure(max_attempts=config.max_retry_attempts)
def check_file_already_processed(file_hash: str) -> bool:
    """
    æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç¶“è™•ç†é
    
    Args:
        file_hash: æ–‡ä»¶çš„é›œæ¹Šå€¼
        
    Returns:
        True å¦‚æœå·²è™•ç†ï¼ŒFalse å¦å‰‡
    """
    if not file_hash:
        return False

    try:
        with safe_session() as session:
            existing_file = session.exec(
                select(ProcessedFile).where(ProcessedFile.file_hash == file_hash)
            ).first()
            return existing_file is not None
    except Exception as e:
        logger.error(f"æª¢æŸ¥æ–‡ä»¶è™•ç†ç‹€æ…‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return False

def save_experiment_to_db(
    experiment_metadata: Dict[str, Any],
    validation_report: Dict[str, Any],
    cell_id: int,
    machine_id: int,
    battery_type: str,
    temperature: float,
    project_id: Optional[int] = None
) -> Experiment:
    """
    å‰µå»ºä¸¦ä¿å­˜æ–°çš„å¯¦é©—è¨˜éŒ„
    
    Args:
        experiment_metadata: å¯¦é©—å…ƒæ•¸æ“š
        validation_report: é©—è­‰å ±å‘Š
        cell_id: é›»æ± å–®å…ƒID
        machine_id: æ©Ÿå™¨ID
        battery_type: é›»æ± é¡å‹
        temperature: å¹³å‡æº«åº¦
        
    Returns:
        å‰µå»ºçš„å¯¦é©—å°è±¡
    """ 
    experiment = Experiment(
        name=experiment_metadata['name'],
        description=experiment_metadata.get('description', ''),
        battery_type=battery_type,
        nominal_capacity=experiment_metadata['nominal_capacity'],
        temperature=temperature,
        operator=experiment_metadata.get('operator', ''),
        start_date=experiment_metadata['start_date'],
        end_date=None,
        validation_status=validation_report['valid'],
        validation_report=validation_report,
        cell_id=cell_id,
        machine_id=machine_id,
        project_id=project_id
    )

    try:
        with safe_session() as session:
            session.add(experiment)
            session.commit()
            session.refresh(experiment)
            return experiment
    except Exception as e:
        raise DatabaseError(f"ä¿å­˜å¯¦é©—è¨˜éŒ„å¤±æ•—: {str(e)}")

def create_measurement_from_row(
    row: pd.Series, 
    step_mapping: Dict[int, int],
    config: ProcessingConfig
) -> Optional[Measurement]:
    """
    å¾æ•¸æ“šè¡Œå‰µå»ºæ¸¬é‡å°è±¡
    
    Args:
        row: æ•¸æ“šè¡Œ
        step_mapping: æ­¥é©Ÿæ˜ å°„
        config: è™•ç†é…ç½®
          Returns:
        æ¸¬é‡å°è±¡æˆ– Noneï¼ˆå¦‚æœç„¡æ³•å‰µå»ºï¼‰
    """
    try:
        step_number = int(row['step_number'])
        if step_number not in step_mapping:
            return None

        step_id = step_mapping[step_number]
        
        # Critical validation: Ensure step_id is not None
        if step_id is None:
            logger.error(f"step_id ç‚º None: step_number={step_number}, step_mapping={step_mapping}")
            return None
        execution_time = float(row['execution_time'])
        voltage = round_numeric_value(row['voltage'], config.voltage_precision)
        current = round_numeric_value(row['current'], config.current_precision)
        temperature = round_numeric_value(
            row.get('temperature', config.default_temperature), 
            config.temperature_precision, 
            config.default_temperature
        )
        capacity = round_numeric_value(row.get('capacity', 0.0), config.capacity_precision)
        energy = round_numeric_value(row.get('energy', 0.0), config.energy_precision)
        return Measurement(
            step_id=step_id,
            execution_time=execution_time,
            voltage=voltage,
            current=current,
            temperature=temperature,
            capacity=capacity,
            energy=energy
            # æ³¨æ„ï¼šæ ¹æ“šæ¥­å‹™éœ€æ±‚ï¼ŒSOC æ•¸æ“šä¸å­˜å„²åˆ°æ¸¬é‡è¡¨ä¸­
        )
    except (ValueError, TypeError) as e:
        logger.warning(f"å‰µå»ºæ¸¬é‡å°è±¡å¤±æ•—: {str(e)}")
        return None

def process_measurements_batch(
    batch_df: pd.DataFrame,
    step_mapping: Dict[int, int],
    config: ProcessingConfig,
    progress_callback: Optional[Callable[[str], None]] = None
) -> tuple[List[Measurement], int, int]:
    """
    è™•ç†æ¸¬é‡æ•¸æ“šæ‰¹æ¬¡
    
    Args:
        batch_df: æ‰¹æ¬¡æ•¸æ“šæ¡†
        step_mapping: æ­¥é©Ÿæ˜ å°„
        config: è™•ç†é…ç½®
        progress_callback: é€²åº¦å›èª¿å‡½æ•¸
        
    Returns:
        (æ¸¬é‡åˆ—è¡¨, è·³éæ•¸é‡, éŒ¯èª¤æ•¸é‡)
    """
    measurements = []
    skipped_count = 0
    error_count = 0
    
    for row_idx, row in batch_df.iterrows():
        if progress_callback:
            progress_callback(f"è™•ç†è¡Œ {row_idx}")
            
        measurement = create_measurement_from_row(row, step_mapping, config)
        
        if measurement is None:
            step_number = int(row.get('step_number', -1))
            if step_number not in step_mapping:
                skipped_count += 1
            else:
                error_count += 1
        else:
            measurements.append(measurement)
    
    return measurements, skipped_count, error_count

def save_measurements_batch(
    session: Session,
    measurements: List[Measurement],
    batch_num: int
) -> None:
    """
    ä¿å­˜æ¸¬é‡æ•¸æ“šæ‰¹æ¬¡åˆ°è³‡æ–™åº«
    
    Args:
        session: è³‡æ–™åº«æœƒè©±
        measurements: æ¸¬é‡åˆ—è¡¨
        batch_num: æ‰¹æ¬¡ç·¨è™Ÿ
    """
    if not measurements:
        logger.info(f"æ‰¹æ¬¡ {batch_num}: æ²’æœ‰æ•¸æ“šéœ€è¦ä¿å­˜")
        return
    
    try:
        # åˆ†å°æ‰¹æ¬¡è™•ç†ï¼Œé¿å…ä¸€æ¬¡æ€§æ’å…¥å¤ªå¤šæ•¸æ“š
        small_batch_size = config.measurement_internal_batch_size # ä½¿ç”¨é…ç½®å€¼
        for i in range(0, len(measurements), small_batch_size):
            small_batch = measurements[i:i + small_batch_size]
            session.add_all(small_batch)
            session.flush()  # ç«‹å³å¯«å…¥é€™å€‹å°æ‰¹æ¬¡
            
            # æ¯å€‹å°æ‰¹æ¬¡å¾ŒçŸ­æš«ç­‰å¾…
            if i + small_batch_size < len(measurements):
                time.sleep(0.01)
        
        logger.info(f"æ‰¹æ¬¡ {batch_num}: æˆåŠŸæº–å‚™ä¿å­˜ {len(measurements)} å€‹æ¸¬é‡æ•¸æ“š")
    except Exception as e:
        session.rollback()
        raise ProcessingError(f"æ‰¹æ¬¡ {batch_num} ä¿å­˜å¤±æ•—: {str(e)}")


def save_measurements_to_db(
    experiment_id: int,
    details_df: pd.DataFrame,
    step_mapping: Dict[int, int],
    nominal_capacity: float,
    batch_size: Optional[int] = None,
    progress_callback: Optional[Callable[[str], None]] = None
) -> None:
    """
    ä¿å­˜æ¸¬é‡æ•¸æ“šåˆ°è³‡æ–™åº«
    
    Args:
        experiment_id: å¯¦é©—ID
        details_df: è©³ç´°æ•¸æ“šæ¡†
        step_mapping: æ­¥é©Ÿæ˜ å°„
        nominal_capacity: æ¨™ç¨±å®¹é‡
        batch_size: æ‰¹æ¬¡å¤§å°
        progress_callback: é€²åº¦å›èª¿å‡½æ•¸
    """
    if details_df.empty:
        logger.warning("æ²’æœ‰æ¸¬é‡æ•¸æ“šéœ€è¦ä¿å­˜")
        return

    batch_size = batch_size or config.default_batch_size
    
    # é©—è­‰å¿…è¦æ¬„ä½
    required_columns = ['step_number', 'execution_time', 'voltage', 'current']
    validate_required_columns(details_df, required_columns)
    
    if not step_mapping:
        raise ValidationError("æ­¥é©Ÿæ˜ å°„ç‚ºç©º")

    logger.info(f"é–‹å§‹è™•ç† {len(details_df)} è¡Œæ•¸æ“šï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
    total_saved_measurements = 0
    total_error_rows = 0
    total_skipped_rows = 0

    for batch_num, i in enumerate(range(0, len(details_df), batch_size), 1):
        batch_df = details_df.iloc[i:i + batch_size]
        
        if progress_callback:
            progress_callback(f"è™•ç†æ‰¹æ¬¡ {batch_num}/{(len(details_df) - 1) // batch_size + 1}")
        
        # Define the operation for a single batch, to be wrapped by retry logic
        def process_and_save_one_batch():
            # This function processes one batch_df and saves it in a new session.
            # It returns (num_saved, num_skipped, num_errors) for this batch.
            
            _measurements, _skipped, _errors = process_measurements_batch(
                batch_df, step_mapping, config, progress_callback
            )
            
            _saved_this_batch = 0
            if _measurements:
                # Each batch gets its own session and transaction
                with safe_session() as session: # Correctly use safe_session for commit/rollback
                    save_measurements_batch(session, _measurements, batch_num)
                    # commit is handled by safe_session
                _saved_this_batch = len(_measurements)
            
            return _saved_this_batch, _skipped, _errors

        try:
            # Apply retry logic to the processing and saving of this single batch
            saved_count, skipped_count, error_count = retry_database_operation(process_and_save_one_batch)
            
            total_saved_measurements += saved_count
            total_skipped_rows += skipped_count
            total_error_rows += error_count
            
        except Exception as e:
            # If a batch fails even after retries, log it and re-raise to stop further processing.
            logger.error(f"æ‰¹æ¬¡ {batch_num} æ°¸ä¹…å¤±æ•—ï¼Œå³ä½¿é‡è©¦å¾Œ: {str(e)}")
            # Wrap the original exception to preserve its details
            raise DatabaseError(f"ä¿å­˜æ¸¬é‡æ•¸æ“šæ™‚ï¼Œæ‰¹æ¬¡ {batch_num} æ°¸ä¹…å¤±æ•—: {str(e)}") from e

    # æœ€çµ‚çµ±è¨ˆ
    logger.info("ä¿å­˜å®Œæˆçµ±è¨ˆ:")
    logger.info(f"æˆåŠŸä¿å­˜: {total_saved_measurements} å€‹æ¸¬é‡æ•¸æ“š")
    logger.info(f"è·³éè¡Œæ•¸: {total_skipped_rows} (åŸå› : step_numberä¸åŒ¹é…æˆ–å‰µå»ºæ¸¬é‡å°è±¡æ™‚å…§éƒ¨éŒ¯èª¤)")
    logger.info(f"éŒ¯èª¤è¡Œæ•¸: {total_error_rows} (åŸå› : å‰µå»ºæ¸¬é‡å°è±¡æ™‚æ•¸æ“šé©—è­‰/è½‰æ›å¤±æ•—)")
    if not details_df.empty: # Check if details_df was empty to avoid division by zero
        success_rate = (total_saved_measurements / len(details_df) * 100) if len(details_df) > 0 else 0.0
        logger.info(f"æˆåŠŸç‡: {success_rate:.1f}%")
    else:
        logger.info("æ²’æœ‰æ•¸æ“šé€²è¡Œè™•ç†ï¼ŒæˆåŠŸç‡ä¸é©ç”¨ã€‚")

def save_steps_to_db(
    experiment_id: int,
    steps_df: pd.DataFrame,
    nominal_capacity: float,
    session: Optional[Session] = None
) -> List[Step]:
    """
    ä¿å­˜æ­¥é©Ÿæ•¸æ“šåˆ°è³‡æ–™åº«
    
    Args:
        experiment_id: å¯¦é©—ID
        steps_df: æ­¥é©Ÿæ•¸æ“šæ¡†
        nominal_capacity: æ¨™ç¨±å®¹é‡
        session: å¯é¸çš„è³‡æ–™åº«æœƒè©±
        
    Returns:
        ä¿å­˜çš„æ­¥é©Ÿå°è±¡åˆ—è¡¨
    """
    if experiment_id is None:
        raise ValueError("experiment_id ä¸èƒ½ç‚º None")
    
    steps = []
    own_session = session is None
    
    if own_session:
        session_context = safe_session()
        session = session_context.__enter__()
    
    try:
        for _, row in steps_df.iterrows():
            row_dict = convert_numpy_types(row.to_dict())
            
            step_number = safe_get_float_from_dict(row_dict, "step_number", 0.0)
            step_type = safe_get_str_from_dict(row_dict, "step_type", "unknown")
            duration = safe_get_float_from_dict(row_dict, "duration", 0.0)
            voltage_start = safe_get_float_from_dict(row_dict, "voltage_start", 0.0)
            voltage_end = safe_get_float_from_dict(row_dict, "voltage_end", 0.0)
            current = safe_get_float_from_dict(row_dict, "current", 0.0)
            capacity = safe_get_float_from_dict(row_dict, "capacity", 0.0)
            energy = safe_get_float_from_dict(row_dict, "energy", 0.0)
            temperature_start = safe_get_float_from_dict(row_dict, "temperature_start", config.default_temperature)
            temperature_end = safe_get_float_from_dict(row_dict, "temperature_end", config.default_temperature)
            soc_start = safe_get_optional_float_from_dict(row_dict, "soc_start")
            soc_end = safe_get_optional_float_from_dict(row_dict, "soc_end")
            start_time = row_dict.get('start_time', None) # ä¿æŒåŸæ¨£ï¼Œå› ç‚ºå®ƒè™•ç† datetime
            end_time = row_dict.get('end_time', None) # ä¿æŒåŸæ¨£ï¼Œå› ç‚ºå®ƒè™•ç† datetime
            c_rate = safe_get_float_from_dict(row_dict, "c_rate", 0.0)
            pre_test_rest_time = safe_get_optional_float_from_dict(row_dict, "pre_test_rest_time")
            
            # æ³¨æ„ï¼šå¦‚æœ start_time æˆ– end_time ç‚º Noneï¼Œå‰‡é»˜èªç‚ºç•¶å‰æ™‚é–“ (datetime.now())ã€‚
            # é€™æ„å‘³è‘—å¦‚æœæºæ•¸æ“šä¸­ç¼ºå°‘é€™äº›æ™‚é–“ï¼Œå°‡è¨˜éŒ„è™•ç†æ™‚é–“è€Œéå¯¦éš›äº‹ä»¶æ™‚é–“ã€‚
            # å¦‚æœéœ€è¦å¯¦éš›äº‹ä»¶æ™‚é–“ä¸”ä¸æ‡‰ç‚º Noneï¼Œå‰‡æ‡‰åœ¨æ­¤è™•æ·»åŠ é©—è­‰æˆ–ä¿®æ”¹ Step æ¨¡å‹ä»¥å…è¨± Noneã€‚
            step = Step(
                experiment_id=experiment_id,
                step_number=int(step_number),
                step_type=step_type,
                start_time=start_time if start_time is not None else datetime.now(),
                end_time=end_time if end_time is not None else datetime.now(),
                duration=duration,
                voltage_start=voltage_start,
                voltage_end=voltage_end,
                current=current,
                capacity=capacity,
                energy=energy,
                temperature_start=temperature_start,
                temperature_end=temperature_end,                c_rate=c_rate,
                soc_start=soc_start,
                soc_end=soc_end,
                pre_test_rest_time=pre_test_rest_time,
                data_meta=row_dict if isinstance(row_dict, dict) else {}
            )

            session.add(step)
            steps.append(step)

        # ç²å–è‡ªå‹•ç”Ÿæˆçš„ ID
        session.flush()
        
        for step in steps:
            session.refresh(step)
        
        # é©—è­‰æ‰€æœ‰æ­¥é©Ÿéƒ½æœ‰æœ‰æ•ˆçš„ ID
        invalid_steps = [step for step in steps if step.id is None]
        if invalid_steps:
            raise DatabaseError(f"ç„¡æ³•ç²å– {len(invalid_steps)} å€‹æ­¥é©Ÿçš„æœ‰æ•ˆ ID")
        
        if own_session:
            session.commit()
            for step in steps:
                session.expunge(step)
                
    except Exception as e:
        logger.error(f"ä¿å­˜æ­¥é©Ÿæ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        if own_session:
            session.rollback()
        raise DatabaseError(f"ä¿å­˜æ­¥é©Ÿæ•¸æ“šå¤±æ•—: {str(e)}")
    finally:
        if own_session:
            session_context.__exit__(None, None, None)
            
    return steps

def save_processed_files_to_db(
    experiment_id: int,
    step_filename: str,
    detail_filename: str,
    step_file_hash: str,
    detail_file_hash: str,
    step_df_len: int,
    detail_df_len: int,
    step_metadata: Dict[str, Any],
    detail_metadata: Dict[str, Any]
) -> None:
    """
    ä¿å­˜å·²è™•ç†æ–‡ä»¶è¨˜éŒ„åˆ°è³‡æ–™åº«
    
    Args:
        experiment_id: å¯¦é©—ID
        step_filename: æ­¥é©Ÿæ–‡ä»¶å
        detail_filename: è©³ç´°æ–‡ä»¶å
        step_file_hash: æ­¥é©Ÿæ–‡ä»¶é›œæ¹Š
        detail_file_hash: è©³ç´°æ–‡ä»¶é›œæ¹Š
        step_df_len: æ­¥é©Ÿæ•¸æ“šæ¡†è¡Œæ•¸
        detail_df_len: è©³ç´°æ•¸æ“šæ¡†è¡Œæ•¸
        step_metadata: æ­¥é©Ÿæ–‡ä»¶å…ƒæ•¸æ“š
        detail_metadata: è©³ç´°æ–‡ä»¶å…ƒæ•¸æ“š
    """
    try:
        with safe_session() as session:
            session.add(ProcessedFile(
                experiment_id=experiment_id,
                filename=step_filename,
                file_type=FILE_TYPE_STEP,  # ä½¿ç”¨å¸¸é‡
                file_hash=step_file_hash,
                row_count=step_df_len,
                data_meta=step_metadata
            ))

            session.add(ProcessedFile(
                experiment_id=experiment_id,
                filename=detail_filename,
                file_type=FILE_TYPE_DETAIL,  # ä½¿ç”¨å¸¸é‡
                file_hash=detail_file_hash,
                row_count=detail_df_len,
                data_meta=detail_metadata
            ))

            session.commit()
    except Exception as e:
        raise DatabaseError(f"ä¿å­˜å·²è™•ç†æ–‡ä»¶è¨˜éŒ„å¤±æ•—: {str(e)}")

def update_experiment_end_date(experiment_id: int, end_time: datetime) -> None:
    """
    æ›´æ–°å¯¦é©—çš„çµæŸæ—¥æœŸ
    
    Args:
        experiment_id: å¯¦é©—ID
        end_time: çµæŸæ™‚é–“
    """
    try:
        with safe_session() as session:
            experiment = session.get(Experiment, experiment_id)
            if experiment:
                experiment.end_date = end_time
                session.add(experiment)
                session.commit()
            else:
                raise ValidationError(f"æ‰¾ä¸åˆ° ID ç‚º {experiment_id} çš„å¯¦é©—")
    except Exception as e:
        raise DatabaseError(f"æ›´æ–°å¯¦é©—çµæŸæ—¥æœŸå¤±æ•—: {str(e)}")

def delete_experiment_and_related(experiment_id: int) -> None:
    """
    [æ¸¬è©¦ç”¨ - ç©ºæ“ä½œ] åˆªé™¤æŒ‡å®šå¯¦é©—åŠå…¶ç›¸é—œçš„ step å’Œ measurementã€‚
    Args:
        experiment_id: è¦åˆªé™¤çš„å¯¦é©— ID
    Raises:
        DatabaseError: åˆªé™¤éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤
    """
    logger.info(f"ğŸ” DEBUG (NO-OP): è«‹æ±‚åˆªé™¤å¯¦é©— ID: {experiment_id}")

    # æ¨¡æ“¬æª¢æŸ¥å¯¦é©—æ˜¯å¦å­˜åœ¨
    # try:
    #     with safe_session() as session:
    #         experiment = session.get(Experiment, experiment_id)
    #         if not experiment:
    #             logger.warning(f"ğŸ” DEBUG (NO-OP): å¯¦é©— ID {experiment_id} è‹¥å¯¦éš›åŸ·è¡Œå‰‡æ‰¾ä¸åˆ°")
    #             # raise DatabaseError(f"å¯¦é©— ID {experiment_id} ä¸å­˜åœ¨") # åœ¨ç©ºæ“ä½œä¸­ä¸æ‹‹å‡º
    #         else:
    #             logger.info(f"ğŸ” DEBUG (NO-OP): è‹¥å¯¦éš›åŸ·è¡Œï¼Œå°‡åˆªé™¤å¯¦é©—: {experiment.name}")
    # except Exception as e:
    #     logger.error(f"ğŸ” DEBUG (NO-OP): æ¨¡æ“¬æª¢æŸ¥å¯¦é©—æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    logger.info(f"ğŸ” DEBUG (NO-OP): æ¨¡æ“¬æŸ¥æ‰¾ç›¸é—œçš„ steps for experiment_id: {experiment_id}")
    # step_ids = [] # æ¨¡æ“¬
    logger.info(f"ğŸ” DEBUG (NO-OP): æ¨¡æ“¬æ‰¾åˆ° 0 å€‹ steps")

    # measurement_count = 0 # æ¨¡æ“¬
    logger.info(f"ğŸ” DEBUG (NO-OP): æ¨¡æ“¬æ‰¾åˆ° 0 å€‹ measurements éœ€è¦åˆªé™¤")

    logger.info("ğŸ” DEBUG (NO-OP): æ¨¡æ“¬åˆ†æ‰¹åˆªé™¤ measurements (å¯¦éš›æœªåŸ·è¡Œ)")
    # deleted_count = 0
    # logger.info(f"ğŸ” DEBUG (NO-OP): æ¨¡æ“¬å·²åˆªé™¤ {deleted_count}/{measurement_count} å€‹ measurements")

    logger.info("ğŸ” DEBUG (NO-OP): æ¨¡æ“¬åˆªé™¤ steps (å¯¦éš›æœªåŸ·è¡Œ)")
    # logger.info(f"ğŸ” DEBUG (NO-OP): æ¨¡æ“¬æº–å‚™åˆªé™¤ 0 å€‹ steps")

    logger.info("ğŸ” DEBUG (NO-OP): æ¨¡æ“¬åˆªé™¤ ProcessedFile è¨˜éŒ„ (å¯¦éš›æœªåŸ·è¡Œ)")
    # logger.info(f"ğŸ” DEBUG (NO-OP): æ¨¡æ“¬æº–å‚™åˆªé™¤ 0 å€‹ ProcessedFile è¨˜éŒ„")
            
    logger.info("ğŸ” DEBUG (NO-OP): æ¨¡æ“¬åˆªé™¤ experiment (å¯¦éš›æœªåŸ·è¡Œ)")
    # logger.info(f"ğŸ” DEBUG (NO-OP): æ¨¡æ“¬æº–å‚™åˆªé™¤ experiment")
            
    logger.info("ğŸ” DEBUG (NO-OP): æ¨¡æ“¬æäº¤äº‹å‹™ (å¯¦éš›æœªåŸ·è¡Œ)")
    logger.info(f"ğŸ” DEBUG (NO-OP): å¯¦é©— ID {experiment_id} çš„åˆªé™¤æ“ä½œå·²è¨˜éŒ„ (æœªå¯¦éš›åŸ·è¡Œè³‡æ–™åº«æ“ä½œ)")
